For Compiling,

gcc -o <output file> <main file>.c

For Running, 

./<output file> <input file>

For checking the time,


# Word Frequency Analyzer: Two Implementation Comparison

## Overview
Both programs solve the same problem (word frequency analysis) but use different approaches for memory management, data structures, and some implementation details.

## Key Architectural Differences

### 1. Memory Management Strategy

**Original (Static Arrays):**
```c
WordFreq words[MAX_WORDS];           // Fixed 100,000 slots
int word_count = 0;                  // Simple counter
#define MAX_WORDS 100000
```

**New (Dynamic Arrays):**
```c
typedef struct {
    Word *array;                     // Dynamically allocated
    int size;                        // Current elements
    int capacity;                    // Current capacity
} WordArray;

void resizeWordArray(WordArray *wa) {
    wa->capacity *= 2;               // Doubles when full
    wa->array = realloc(wa->array, sizeof(Word) * wa->capacity);
}
```

**Trade-offs:**
- **Static**: Simple, fast allocation, but wastes memory and has hard limits
- **Dynamic**: Memory efficient, unlimited growth, but adds complexity and reallocation overhead

### 2. Data Structure Design

**Original:**
```c
typedef struct {
    char word[MAX_WORD_LEN];
    int count;
} WordFreq;
```

**New:**
```c
typedef struct {
    char word[MAX_WORD_LEN];
    int count;
} Word;

typedef struct {
    Word *array;
    int size;
    int capacity;
} WordArray;
```

The new version adds an abstraction layer with `WordArray` struct, providing better encapsulation.

### 3. File Processing Methods

**Original (Line-based with strtok):**
```c
char buffer[BUFFER_SIZE];
while(fgets(buffer, sizeof(buffer), fp)) {
    char *token = strtok(buffer, " \t\n\r");
    while(token) {
        clean_word(token);
        if(strlen(token) > 0 && !is_stop_word(token))
            insert_word(token);
        token = strtok(NULL, " \t\n\r");
    }
}
```

**New (Word-based with fscanf):**
```c
char word[MAX_WORD_LEN];
while (fscanf(fp, "%99s", word) == 1) {
    toLowerStr(word);
    removePunct(word);
    insertWord(wa, word);
}
```

**Trade-offs:**
- **Original**: More robust, handles long lines, uses buffered I/O
- **New**: Simpler code, but less efficient for large files and vulnerable to very long words

### 4. String Processing Approaches

**Original (In-place cleaning):**
```c
void clean_word(char *w) {
    int j=0;
    for(int i=0; w[i]; i++) {
        if(isalpha(w[i])) w[j++] = tolower(w[i]);
    }
    w[j] = '\0';
}
```

**New (Separate functions):**
```c
void toLowerStr(char *str) {
    for (int i = 0; str[i]; i++)
        str[i] = tolower(str[i]);
}

void removePunct(char *str) {
    int i, j = 0;
    for (i = 0; str[i]; i++) {
        if (isalnum(str[i])) str[j++] = str[i];  // Note: isalnum vs isalpha
    }
    str[j] = '\0';
}
```

**Key Difference**: New version uses `isalnum()` (allows digits) vs original's `isalpha()` (letters only).

### 5. Binary Search Implementation

**Original:**
```c
int binary_search(char *w, int low, int high) {
    while(low <= high) {
        int mid = (low + high)/2;
        int cmp = strcmp(words[mid].word, w);
        if(cmp == 0) return mid;
        else if(cmp < 0) low = mid +1;
        else high = mid -1;
    }
    return -1;
}
```

**New:**
```c
int binarySearch(WordArray *wa, char *word, int *insertPos) {
    int left = 0, right = wa->size - 1;
    while (left <= right) {
        int mid = left + (right - left)/2;        // Overflow-safe
        int cmp = strcmp(word, wa->array[mid].word);
        if (cmp == 0) return mid;
        else if (cmp < 0) right = mid - 1;
        else left = mid + 1;
    }
    *insertPos = left;                           // Returns insertion position
    return -1;
}
```

**New version improvements:**
- Overflow-safe midpoint calculation
- Returns insertion position via pointer parameter
- More descriptive variable names

### 6. Output Format

**Original:**
```c
printf("%-15s | ", words[i].word);
for(int j=0; j<words[i].count && j<50; j++) printf("*");
printf(" (%d)\n", words[i].count);
```
Output: `word            | ***** (5)`

**New:**
```c
printf("%-15s | %4d | ", wa->array[i].word, wa->array[i].count);
int stars = wa->array[i].count > 50 ? 50 : wa->array[i].count;
for (int j = 0; j < stars; j++) printf("*");
printf("\n");
```
Output: `word            |    5 | *****`

## Performance Comparison

### Time Complexity
Both implementations have the same overall complexity:
- **Word insertion**: O(log n) search + O(n) shifting = O(n)
- **Overall**: O(w × log w + w × n) where w = unique words, n = average position
- **Final sorting**: O(w log w)

### Space Complexity
- **Original**: O(MAX_WORDS) - always allocates full array
- **New**: O(w) - only allocates what's needed, grows as needed

### Memory Efficiency Test
```c
// Original: Always uses ~10MB (100,000 × 104 bytes)
// New: Uses ~10KB initially, grows to actual size needed
```

## Code Quality Analysis

### Strengths of Original Version:
✅ **Robust file processing** with buffered I/O  
✅ **Simple memory management** - no malloc/free complexity  
✅ **Better error handling** for large inputs  
✅ **More efficient** for very large datasets (no reallocation overhead)  

### Strengths of New Version:
✅ **Memory efficient** - uses only what's needed  
✅ **Better encapsulation** with WordArray struct  
✅ **More descriptive** function and variable names  
✅ **Overflow-safe** binary search  
✅ **Cleaner separation** of concerns (separate lowercase/punctuation functions)  
✅ **Professional coding style** with proper initialization  

### Weaknesses of Original:
❌ **Memory waste** - always allocates 10MB  
❌ **Hard limits** - MAX_WORDS constraint  
❌ **Less readable** variable names  

### Weaknesses of New Version:
❌ **Less robust file processing** - fscanf limitations  
❌ **Memory management complexity** - potential for memory leaks  
❌ **Reallocation overhead** during growth  
❌ **Different character handling** (isalnum vs isalpha)  

## Benchmark Scenarios

### Small Dataset (< 1000 unique words):
- **Winner**: New version (uses ~100KB vs 10MB)

### Large Dataset (50,000+ unique words):
- **Winner**: Original (no reallocation overhead, better I/O)

### Memory-Constrained Environment:
- **Winner**: New version (grows as needed)

### High-Performance Requirements:
- **Winner**: Original (predictable memory access, no reallocation)

## Recommendations

**Choose Original When:**
- Processing very large files (GB+)
- Memory is abundant
- Performance is critical
- You need robust text parsing

**Choose New When:**
- Processing many small files
- Memory usage matters
- You need unlimited word capacity
- Code maintainability is important

## Hybrid Approach
For the best of both worlds:
```c
// Start with reasonable initial capacity
#define INITIAL_CAPACITY 10000
// Use buffered I/O from original
// Use dynamic growth from new version
// Use overflow-safe binary search from new version
```

Both implementations are well-designed for their intended use cases, with the original focusing on performance and robustness, while the new version emphasizes memory efficiency and code organization.

